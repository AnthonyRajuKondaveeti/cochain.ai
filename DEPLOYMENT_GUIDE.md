# CoChain.ai Deployment Guide
## Deploy to Render (Free Tier - 512MB RAM)

This guide walks you through deploying CoChain.ai to Render's free tier using the lightweight configuration that offloads ML inference to HuggingFace's API.

---

## üìã Prerequisites

1. **GitHub Account** - Code repository
2. **Render Account** - Sign up at [render.com](https://render.com)
3. **Supabase Account** - Your existing database
4. **HuggingFace Account** - For free embedding API access

---

## üîë Step 1: Get HuggingFace API Key

1. Go to [huggingface.co](https://huggingface.co)
2. Sign up/login (free account)
3. Navigate to **Settings** ‚Üí **Access Tokens**
4. Click **New Token**
   - Name: `CoChain-Production`
   - Type: `Read` (default)
5. Copy the token (starts with `hf_...`)

**Free Tier Limits:**
- 30,000 requests/month (~1,000/day)
- Rate limit: ~1 request/second
- Sufficient for small-medium traffic with caching

---

## üöÄ Step 2: Deploy to Render

### Option A: Blueprint Deployment (Recommended)

1. **Push Code to GitHub**
   ```powershell
   git add .
   git commit -m "Add deployment configuration for 512MB RAM"
   git push origin main
   ```

2. **Create Render Service**
   - Go to [Render Dashboard](https://dashboard.render.com)
   - Click **New** ‚Üí **Blueprint**
   - Connect your GitHub repository
   - Select `For_deploying_CoChain` repository
   - Render will detect `render.yaml` automatically
   - Click **Apply**

3. **Configure Environment Variables**
   
   Render will prompt for these variables:
   
   ```bash
   # Supabase Configuration
   SUPABASE_URL=https://your-project.supabase.co
   SUPABASE_KEY=eyJxxx...  # Anon/public key
   SUPABASE_SERVICE_KEY=eyJxxx...  # Service role key
   
   # HuggingFace API (for embeddings)
   HUGGINGFACE_API_KEY=hf_xxx...
   
   # Flask Configuration
   SECRET_KEY=<auto-generated>  # Render generates this
   FLASK_ENV=production
   ```

4. **Deploy**
   - Click **Create Web Service**
   - Wait 5-10 minutes for initial build
   - Watch build logs for errors

### Option B: Manual Deployment

1. **Create Web Service**
   - Go to Render Dashboard
   - Click **New** ‚Üí **Web Service**
   - Connect GitHub repository

2. **Configure Build Settings**
   ```
   Name: cochain-ai
   Runtime: Python 3
   Region: Oregon (or closest to users)
   Branch: main
   Build Command: pip install -r requirements-light.txt
   Start Command: gunicorn app:app --workers 1 --threads 4 --timeout 120 --bind 0.0.0.0:$PORT
   ```

3. **Set Environment Variables** (same as Option A)

4. **Choose Free Plan**
   - Instance Type: **Free** (512MB RAM)
   - Click **Create Web Service**

---

## üîç Step 3: Verify Deployment

### Check Logs

1. Go to your Render service
2. Click **Logs** tab
3. Look for these startup messages:
   ```
   ‚úÖ Using HuggingFace API for embeddings (low RAM mode)
   ‚úÖ RL Recommendation Engine initialized successfully
   ‚úÖ Running in PRODUCTION mode
   ```

### Test Endpoints

1. **Health Check**
   ```
   https://cochain-ai.onrender.com/
   ```
   Should show the landing page

2. **Login/Register**
   ```
   https://cochain-ai.onrender.com/register
   https://cochain-ai.onrender.com/login
   ```

3. **Dashboard (after login)**
   ```
   https://cochain-ai.onrender.com/dashboard
   ```
   First load may be slow (~30s cold start + API call)
   Subsequent loads should be fast (<2s with caching)

---

## ‚öôÔ∏è Configuration Details

### Memory Usage (After Deployment)

```
Component                   RAM Usage
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Python Runtime              ~50MB
Flask + Dependencies        ~80MB
Supabase Client            ~30MB
NumPy                      ~50MB
SciPy                      ~80MB
Application Code           ~20MB
OS Overhead                ~50MB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL                      ~360MB
FREE                       ~152MB
```

### Performance Characteristics

**First Request (Cache Miss):**
- Cold start (free tier): ~30 seconds
- Embedding API call: 200-500ms
- Similarity calculation: 50-100ms
- Total: 30-35 seconds

**Subsequent Requests (Cache Hit):**
- No cold start (if active): 0ms
- No embedding needed: 0ms
- Database lookup: 50-100ms
- Total: 100-200ms

**Cache Hit Rate:**
- New users: 0% (first visit)
- Returning users: 95%+ (profile unchanged)
- Average: ~80% across all requests

### A/B Testing Integrity

‚úÖ **Preserved** - All A/B testing logic runs locally:
- User assignment: `ab_test_service.py` (hash-based)
- Thompson Sampling: `contextual_bandit.py` (NumPy only)
- Statistical tests: `scipy.stats` (local)

Only embedding generation is external - test integrity maintained.

---

## üêõ Troubleshooting

### Issue: Build Fails - "ModuleNotFoundError: sentence_transformers"

**Solution:** Ensure `requirements-light.txt` is being used, not `requirements.txt`

Check `render.yaml`:
```yaml
buildCommand: pip install -r requirements-light.txt
```

### Issue: "HUGGINGFACE_API_KEY not set" in Logs

**Solution:** Add environment variable in Render dashboard

1. Go to service ‚Üí **Environment**
2. Add variable: `HUGGINGFACE_API_KEY=hf_xxx...`
3. Click **Save Changes** (triggers redeploy)

### Issue: 503 Errors on First Embedding Request

**Expected Behavior:** HuggingFace API loads model on first request (~30s)

The `embeddings_api.py` has built-in retry logic (3 attempts, exponential backoff)

No action needed - subsequent requests will be fast.

### Issue: Service Sleeps After 15 Minutes

**Free Tier Limitation:** Services sleep after 15 min inactivity

- Cold start: 20-30 seconds
- Consider upgrading to paid tier ($7/mo) for always-on service
- Or use [UptimeRobot](https://uptimerobot.com) to ping every 14 min

### Issue: Rate Limit Errors (429)

**Solution:** HuggingFace free tier rate limits

With 80% cache hit rate:
- 1000 daily users ‚Üí 200 API calls/day (well under limit)

If exceeding limits:
- Upgrade to HuggingFace Pro ($9/mo unlimited)
- Or implement more aggressive caching

### Issue: Memory Errors (RAM > 512MB)

**Diagnostic:** Check logs for OOM (Out of Memory) errors

**Solutions:**
1. Verify `requirements-light.txt` is installed (not `requirements.txt`)
2. Reduce gunicorn workers to 1 (already configured)
3. Check for memory leaks in custom code
4. Monitor with: `psutil.virtual_memory()` in code

---

## üìä Monitoring

### Render Metrics (Built-in)

- **Dashboard** ‚Üí Your Service ‚Üí **Metrics**
  - RAM usage
  - CPU usage
  - Request latency
  - Error rate

### Application Logs

```python
# Check embedding mode on startup
logger.info("‚úÖ Using HuggingFace API for embeddings")

# Monitor API calls
logger.debug("‚úÖ Embedding generated: 384 dimensions")
logger.debug("‚úÖ Cache hit for text: ...")
```

### HuggingFace Usage

- [HuggingFace Dashboard](https://huggingface.co/settings/tokens)
- View API usage statistics
- Monitor rate limits

---

## üîÑ Updating Deployment

### Code Changes

```powershell
# Make changes locally
git add .
git commit -m "Update feature X"
git push origin main
```

Render auto-deploys on push to `main` branch (configured in `render.yaml`).

### Environment Variables

1. Render Dashboard ‚Üí Service ‚Üí **Environment**
2. Edit variables
3. Click **Save Changes** ‚Üí Triggers redeploy

### Rollback

1. Render Dashboard ‚Üí Service ‚Üí **Deploys**
2. Find previous successful deploy
3. Click **‚ãÆ** ‚Üí **Redeploy**

---

## üí∞ Cost Optimization

### Free Tier Limits

| Resource | Free Tier | Paid Tier ($7/mo) |
|----------|-----------|-------------------|
| RAM | 512MB | 1GB+ |
| CPU | Shared | Dedicated |
| Sleep | 15 min inactivity | Always on |
| Build Minutes | 500/mo | Unlimited |
| Hours | 750/mo (or 100 without card) | Unlimited |

### Recommendations

**Start Free:**
- Perfect for MVP/testing
- Add credit card for 750 hrs/mo (vs 100)
- No charges unless you upgrade

**Upgrade When:**
- Cold starts hurt UX (>100 daily users)
- Need <1s response times
- Require 99.9% uptime

**Alternative Free Tiers:**
- **Railway**: 500 hrs/mo, $5 credit
- **Fly.io**: 3 VMs free (256MB each)
- **PythonAnywhere**: Free tier (CPU limits)

---

## üîê Security Checklist

‚úÖ **Environment Variables**
- All secrets in environment (not code)
- `SECRET_KEY` auto-generated by Render
- Supabase keys secure

‚úÖ **HTTPS**
- Render provides free SSL
- `SESSION_COOKIE_SECURE=True` in production

‚úÖ **Database Security**
- Supabase Row Level Security (RLS) enabled
- Service key only for admin operations

‚úÖ **API Keys**
- HuggingFace key limited to `Read` scope
- Rate limiting on all endpoints

---

## üìà Scaling Path

### Phase 1: Free Tier (Current)
- Users: 0-100
- RAM: 512MB
- Cost: $0

### Phase 2: Paid Instance
- Users: 100-1000
- RAM: 1GB (Starter: $7/mo)
- Always-on, no cold starts

### Phase 3: Horizontal Scaling
- Users: 1000-10000
- Multiple instances (load balanced)
- RAM: 2GB per instance ($25/mo each)

### Phase 4: Dedicated Embedding Service
- Users: 10000+
- Deploy own embedding server
- GPU instance for fast inference
- Move back to local sentence-transformers

---

## üÜò Support

### Render Support
- Docs: [render.com/docs](https://render.com/docs)
- Community: [community.render.com](https://community.render.com)
- Status: [status.render.com](https://status.render.com)

### HuggingFace Support
- Docs: [huggingface.co/docs](https://huggingface.co/docs)
- Forum: [discuss.huggingface.co](https://discuss.huggingface.co)

### CoChain.ai Logs
- Check `logs/app/cochain.log.1` for detailed errors
- Enable debug logging: `logger.setLevel(logging.DEBUG)`

---

## ‚úÖ Deployment Checklist

**Pre-Deployment:**
- [ ] Code pushed to GitHub
- [ ] `requirements-light.txt` verified
- [ ] HuggingFace API key obtained
- [ ] Supabase credentials ready

**Deployment:**
- [ ] Render service created
- [ ] Environment variables set
- [ ] Build completed successfully
- [ ] Service shows "Live"

**Post-Deployment:**
- [ ] Health check passes
- [ ] Login/register works
- [ ] Dashboard loads recommendations
- [ ] Logs show API mode enabled
- [ ] No memory errors in metrics

**Testing:**
- [ ] Create account
- [ ] Fill profile
- [ ] View dashboard (first load slow, expected)
- [ ] Refresh dashboard (fast, cached)
- [ ] Test A/B testing (control vs treatment)
- [ ] Check admin analytics

---

## üéâ Success Criteria

Your deployment is successful when:

1. ‚úÖ Service is **Live** on Render
2. ‚úÖ Logs show: `‚úÖ Using HuggingFace API for embeddings (low RAM mode)`
3. ‚úÖ RAM usage stable at ~360MB (below 512MB limit)
4. ‚úÖ Dashboard recommendations load (may be slow first time)
5. ‚úÖ Subsequent loads are fast (<2s)
6. ‚úÖ A/B testing assigns users correctly
7. ‚úÖ No errors in logs after 10 min runtime

---

**Deployment Complete! üöÄ**

Your CoChain.ai platform is now running on Render's free tier with:
- ‚úÖ 512MB RAM footprint (was 1.6GB)
- ‚úÖ Full A/B testing functionality
- ‚úÖ Thompson Sampling RL system
- ‚úÖ Automatic retries and caching
- ‚úÖ Production-ready security
- ‚úÖ $0/month cost

Next Steps:
1. Monitor initial traffic
2. Gather A/B test results
3. Optimize cache hit rates
4. Consider paid tier when needed
